---
---

@string{aps = {American Physical Society,}}

@article{hu2024ADAS,
bibtex_show={true},
title={Automated Design of Agentic Systems},
author={Hu, Shengran and Lu, Cong and Clune, Jeff},
journal={International Conference on Learning Representations},
abbr={(ICLR)},
preview={ADAS.png},
selected={true},
code={https://github.com/ShengranHu/ADAS},
html={https://www.shengranhu.com/ADAS/},
arxiv={2408.08435},
year={2025},
award={üèÜ Outstanding Paper (NeurIPS 2024 Open-World Agent Workshop)},
}


@article{lu2025ACD,
    bibtex_show={true},
    title={Automated Capability Discovery via Foundation Model Self-Exploration},
    author={Lu*, Cong and Hu*, Shengran and Clune, Jeff},
    journal={arXiv preprint arXiv:2502.07577},
    arxiv={2502.07577},
    year={2025},
    selected={true},
    code={https://github.com/conglu1997/ACD/},
    html={https://www.conglu.co.uk/acd/},
    preview={ACD.png},
}

@article{lu2025ACD,
    bibtex_show={true},
    title = {The {AI} Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search},
    author = {Yamada, Yutaro and Lange, Robert Tjarko and Lu, Cong and Hu, Shengran and Lu, Chris and Foerster, Jakob and Clune, Jeff and Ha, David},
    journal={arXiv preprint arXiv:2504.08066},
    arxiv={2504.08066},
    year={2025},
    selected={true},
    code={https://github.com/SakanaAI/AI-Scientist-v2},
    html={https://sakana.ai/ai-scientist-first-publication/},
    preview={AISCI-v2.png},
}

@article{lu2024IntelligentGoExplore,
bibtex_show={true},
title={{Intelligent Go-Explore}: Standing on the Shoulders of Giant Foundation Models},
author={Lu, Cong, and Hu, Shengran and Clune, Jeff},
journal={International Conference on Learning Representations},
abbr={(ICLR)},
year={2025},
arxiv={2405.15143},
selected={true},
code={https://github.com/conglu1997/intelligent-go-explore/tree/main},
html={https://www.conglu.co.uk/intelligentgoexplore/},
preview={ige_framework.png},
}

@article{hu2023ThoughtCloning,
  bibtex_show={true},
  title={{Thought Cloning}: Learning to Think while Acting by Imitating Human Thinking},
  author={Shengran Hu and Jeff Clune},
  abbr={(NeurIPS)},
  award={Spotlight (top 3.1% in 12,343)},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  abstract="Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent's thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents how to think as well as behave, Thought Cloning creates safer, more powerful agents.",
  selected={true},
  preview={Thought_Cloning.gif},
  arxiv={2306.00323},
  code={https://github.com/ShengranHu/Thought-Cloning},
  html={https://www.shengranhu.com/ThoughtCloning/},
}





@InProceedings{hu2021RWE,
bibtex_show={true},
author="Hu, Shengran
and Cheng, Ran
and He, Cheng
and Lu, Zhichao
and Jing Wang
and Miao Zhang",
title="Accelerating Multi-Objective Neural Architecture Search by Random-Weight Evaluation",
booktitle="Complex & Intelligent Systems",
year="2021",
abstract="For the goal of automated design of high-performance deep convolutional neural networks (CNNs), Neural Architecture Search (NAS) methodology is becoming increasingly important for both academia and industries. Due to the costly stochastic gradient descent (SGD) training of CNNs for performance evaluation, most existing NAS methods are computationally expensive for real-world deployments. To address this issue, we first introduce a new performance estimation metric, named Random-Weight Evaluation (RWE) to quantify the quality of CNNs in a cost-efficient manner. Instead of fully training the entire CNN, the RWE only trains its last layer and leaves the remainders with randomly initialized weights, which results in a single network evaluation in seconds. Second, a complexity metric is adopted for multi-objective NAS to balance the model size and performance. Overall, our proposed method obtains a set of efficient models with state-of-the-art performance in two real-world search spaces. Then the results obtained on the CIFAR-10 dataset are transferred to the ImageNet dataset to validate the practicality of the proposed algorithm. Moreover, ablation studies on NAS-Bench-301 datasets reveal the effectiveness of the proposed RWE in estimating the performance compared with existing methods.",

pdf={https://link.springer.com/content/pdf/10.1007/s40747-021-00594-5.pdf},
preview={CAIS2021.jpg},
}

@InProceedings{hu2021AlmostNoTraining,
bibtex_show={true},
author="Hu, Shengran
and Cheng, Ran
and He, Cheng
and Lu, Zhichao",
title="Multi-objective Neural Architecture Search with Almost No Training",
booktitle="Evolutionary Multi-Criterion Optimization",
year="2021",
publisher="Springer International Publishing",
pages="492--503",
abstract="In the recent past, neural architecture search (NAS) has attracted increasing attention from both academia and industries. Despite the steady stream of impressive empirical results, most existing NAS algorithms are computationally prohibitive to execute due to the costly iterations of stochastic gradient descent (SGD) training. In this work, we propose an effective alternative, dubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of network architectures. By just training the last linear classification layer, RWE reduces the computational cost of evaluating an architecture from hours to seconds. When integrated within an evolutionary multi-objective algorithm, RWE obtains a set of efficient architectures with state-of-the-art performance on CIFAR-10 with less than two hours' searching on a single GPU card. Ablation studies on rank-order correlations and transfer learning experiments to ImageNet have further validated the effectiveness of RWE.",
preview={emo2021.jpg},
arxiv={2011.13591},
selected={true},
}

